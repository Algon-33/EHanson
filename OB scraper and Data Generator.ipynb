{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Ali\\\\Documents\\\\Auto Hanson'"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Notes:\n",
    "#Shortned training is what was actually used for finetuning GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Ali\\\\Documents\\\\Auto Hanson'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set the seed to make things replicable\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import json\n",
    "import random\n",
    "random.seed(0)\n",
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'why-not-impossible-worlds'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use this to turn a string into an acceptable substring of a filename\n",
    "def slugify(value, allow_unicode=False):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
    "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
    "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
    "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
    "    trailing whitespace, dashes, and underscores.\n",
    "    \"\"\"\n",
    "    value = str(value)\n",
    "    if allow_unicode:\n",
    "        value = unicodedata.normalize('NFKC', value)\n",
    "    else:\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
    "    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n",
    "slugify('Why Not Impossible Worlds?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Gets all the URLS for each months posts on OvercomingBias. You need the headers arg as otherwise the page will reject \n",
    "#the request. To find such headers, go to developer tools, network, GET and look for a string like below.\n",
    "def get_month_archives():\n",
    "\n",
    "    URL = \"https://www.overcomingbias.com/archives\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:106.0) Gecko/20100101 Firefox/106.0\"}\n",
    "    page = requests.get(URL, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    month_archives = list(map(lambda x: x.get(\"href\"), soup.find(\"div\", {\"id\": \"monthly-archives\"}).find_all(\"a\")))\n",
    "    return month_archives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_OB_from_month_archives(month_archives, path):\n",
    "    for i in range(len(month_archives)):\n",
    "        month_URL = month_archives[i] \n",
    "        print(month_URL)\n",
    "        ls = get_articles(get_month_links(month_URL))\n",
    "        save_articles_from_article_list(ls, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_articles_from_article_list(article_list, path):\n",
    "    \n",
    "    \n",
    "    for article in article_list:\n",
    "        os.chdir(path+\"\\\\Articles\")\n",
    "        with open(slugify(article[\"title\"])+\".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for k in article:\n",
    "                f.write(k + \": \" +article[k]+\"\\n\\n\")\n",
    "            f.close()\n",
    "    os.chdir(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_month_links(month_URL):\n",
    "\n",
    "    page = 1\n",
    "    flag = True\n",
    "    links_page = []\n",
    "    while flag:\n",
    "        \n",
    "        page_n = requests.get(month_URL+\"/page/\"+str(page), headers=headers)\n",
    "        \n",
    "        soup_N = BeautifulSoup(page_n.content, \"html.parser\")\n",
    "        \n",
    "\n",
    "        links_page += list(map(lambda x: x.find(\"a\").get(\"href\"), soup_N.find_all(\"h2\", {\"class\": \"entry-title\"})))\n",
    "        \n",
    "        try:\n",
    "            if soup_N.find(\"div\", {\"class\":\"nav-next\"}).find(\"a\") != None or soup_N.find(\"dive\", {\"class\": \"nav-next\"}) ==None:\n",
    "                page +=1\n",
    "        except: \n",
    "            flag = False\n",
    "            \n",
    "    return links_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_articles(month_urls):\n",
    "\n",
    "    articles = []\n",
    "    for link in month_urls:\n",
    "        article_page = requests.get(link, headers = headers)\n",
    "        article_soup = BeautifulSoup(article_page.content, \"html.parser\")\n",
    "\n",
    "        title, author, date = get_title_author_date(article_soup)\n",
    "        if (title, author, date ) !=(None, None, None):\n",
    "            text = article_soup.find(\"div\", {\"class\": \"entry-content\"}).get_text()\n",
    "            text = text.replace(\"\\nGD Star Ratingloading...\", \"\")\n",
    "            articles.append({\"title\": title,\"author\":author, \"date\":date, \"text\":text})\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_title_author_date(soup):\n",
    "    try:\n",
    "        title = soup.find(\"h1\", {\"class\":\"entry-title\"}).get_text()\n",
    "        author = soup.find(\"span\", {\"class\":\"author vcard\"}).find(\"a\").get_text()\n",
    "        date = soup.find(\"span\", {\"class\":\"entry-date\"}).get_text()\n",
    "    except:\n",
    "        title, author, date = (None, None, None)\n",
    "    return (title, author, date)\n",
    "#get_title_author_date(article_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"classes_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "#temp = lambda txt: clean_txt(\"!| [Oo]btained|Level |levelled|[0-9]+|.*[\\—\\-\\–].*|.*[Ss]kill.*\" ,\"\", txt)\n",
    "def temp_1(lines):\n",
    "    temp = lambda txt: clean_txt(\".*Rank  Horror.*|Temporary|.*:.*|[Rr]emoved|Class|.*Boon of the.*|.*\\(.*|Aspect.*|\\.|My Noble Virtue:.*|Bound Spell:.*|Class Consolidation Failed. Unable to Advance|^\\[\\\\n$|^\\[\\].*|^\\[$|Class Consolidation: |Spell [\\—\\-\\–].*|Condition.*|Conditions Met.*|.*Miracle.*|!| [Oo]btained|Level |levelled|[0-9]+|.*[\\—\\-\\–]{1}\\]|.*[Ss]kill.*\" ,\"\", txt)\n",
    "    remove_extra_spaces = lambda txt: clean_txt(\"\\[ \",\"[\" ,clean_txt(\" \\]\", \"]\", txt))\n",
    "    x = [];\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i]\n",
    "        if temp(line) != \"\\n\":\n",
    "            x.append(remove_extra_spaces(temp(line)))\n",
    "    return x\n",
    "new_lines = list(map(lambda x: x+\"\\n\",  list(set(map( lambda x: x.strip(\"\\n\"), temp_1(lines))).difference(spell_set).difference(skill_set))))\n",
    "with open(\"classes_cleaned.txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "    f.write(\"\".join(new_lines))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Uncoment to download OB hosted articles. \n",
    "#If an article redirects to another website, it won't be included.\n",
    "#scrape_OB_from_month_archives(get_month_archives(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_text_dict(txt):\n",
    "    ls = [ i.split(\": \",1) for i in  txt.split(\"\\n\\n\")]\n",
    "    k_v_list = []\n",
    "    for i in ls:\n",
    "        if len(i)==2:\n",
    "            k,v = i\n",
    "            \n",
    "            k = k.strip(\"\\n\").strip(\" \")\n",
    "            k_v_list.append([k,v])\n",
    "    return  dict(k_v_list)\n",
    "#assert get_text_dict(\"text: Hi \\n\\n Author: me\") == {' Author': 'me', 'text': 'Hi '}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seperator = \"\\n\\n###\\n\\n\"\n",
    "\n",
    "def get_Date_Title_data(txt_dict, seperator, stop_sequence):\n",
    "    return {\"prompt\": \"Date: \" + txt_dict[\"date\"] +seperator, \"completion\":\" \" +\"Title: \" +txt_dict[\"title\"] + stop_sequence}\n",
    "    \n",
    "        \n",
    "def get_DateTitle_Intro_data(txt_dict,chunked_text, seperator, stop_sequence):\n",
    "    completion = chunked_text[0]    \n",
    "    if completion[0]!=\" \":\n",
    "        completion = \" \"+completion\n",
    "    completion += stop_sequence            \n",
    "    \n",
    "    prompt = \"Date: \" + txt_dict[\"date\"] + \" \\n\" + \"Title: \" +txt_dict[\"title\"] + seperator\n",
    "    \n",
    "    return {\"prompt\": prompt, \"completion\": completion}\n",
    "\n",
    "def get_essay_data(my_dict, chunked_text, seperator, stop_sequence):\n",
    "    ls = []\n",
    "    for i in range(len(chunked_text)-1):\n",
    "        if i ==0:\n",
    "            prompt = \"Date: \" + my_dict[\"date\"] + \" \\n\" + \"Title: \" +my_dict[\"title\"]+ \"\\n\" + chunked_text[i]+seperator\n",
    "        else:\n",
    "            prompt = chunked_text[i] + seperator\n",
    "        completion = \" \"+chunked_text[i+1]+stop_sequence\n",
    "        if completion[0]!=\" \":\n",
    "            completion += \" \"\n",
    "        ls.append({\"prompt\": prompt, \"completion\": completion})\n",
    "    return ls\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def chunk_text(my_dict):\n",
    "    text = my_dict[\"text\"]\n",
    "    lines = text.split(\"\\n\")\n",
    "    txt = \"\";\n",
    "    intro = \"\";\n",
    "    chunked_text = []\n",
    "    for i in lines:\n",
    "        if len(txt  +i)>500*4 and intro == \"\":\n",
    "            intro = txt\n",
    "            chunked_text.append(intro)\n",
    "            txt = \"\"\n",
    "            \n",
    "        elif len(txt+i)>2000*2:\n",
    "            chunked_text.append(txt)\n",
    "            txt = \"\"\n",
    "        txt+=i\n",
    "        \n",
    "    if txt !=\"\":\n",
    "        chunked_text.append(txt)\n",
    "\n",
    "    return chunked_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_data_to_destination(path, destination, article_name, sep, stop_seq):\n",
    "    os.chdir(path+\"\\Articles\")\n",
    "    with open(article_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    f.close()\n",
    "    os.chdir(path)\n",
    "    text = text.replace(\"\\n\\n\\n\", \"\\n\\n\")\n",
    "    \n",
    "    my_dict = get_text_dict(text)\n",
    "    if my_dict[\"author\"].lower() != \"robin hanson\":\n",
    "        return 0;\n",
    "    \n",
    "    DTd = get_Date_Title_data(my_dict, sep, stop_seq)\n",
    "    chunked_text = chunk_text(my_dict)\n",
    "    DTId = get_DateTitle_Intro_data(my_dict,chunked_text,  sep, stop_seq)\n",
    "    Ed = get_essay_data(my_dict, chunked_text, sep, stop_seq)\n",
    "    \n",
    "    \n",
    "    with open(destination + \".jsonl\",\"a\", encoding=\"utf-8\") as f:\n",
    "        json.dump(DTd, f)\n",
    "        f.write(\"\\n\")\n",
    "        json.dump(DTId, f)\n",
    "        f.write(\"\\n\")\n",
    "        for i in Ed:\n",
    "            json.dump(i, f)\n",
    "            f.write(\"\\n\")\n",
    "    f.close()\n",
    "def generate_train_and_val_files_from_article_list(path, article_list, sep, stop_seq):\n",
    "    for i in article_list:\n",
    "        try:\n",
    "            x = random.uniform(0,1)\n",
    "            if x<0.0125:\n",
    "\n",
    "                write_data_to_destination(path, \"validation\", i, sep, stop_seq)\n",
    "            elif x<0.25:\n",
    "                write_data_to_destination(path, \"training\", i, sep, stop_seq)\n",
    "            else:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "#If the folder where all the articles are is NOT Articles, then you need to modify this function to chdir to the write folder\n",
    "#If the folder where all the articles are has non .txt files, you need to modify this function so that articles is a list\n",
    "#of only .txt files\n",
    "def generate_training_and_val_data(path, sep, stop_seq):\n",
    "    os.chdir(path+\"\\Articles\")\n",
    "    #This is to remove the ipynb checkpoint folder.\n",
    "    articles = os.listdir()[1:]\n",
    "    os.chdir(path)\n",
    "    generate_train_and_val_files_from_article_list(path, articles, sep, stop_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sep = \"\\n\\n###\\n\\n\"\n",
    "stop_seq = \"\\n\\n@@@\\n\\n\"\n",
    "\n",
    "random.seed(0)\n",
    "#IMPORTANT BEFORE YOU RUN THIS, RUN RANDOM.SEED(0) OR ELSE YOU'LL GET A DIFFERENT TRAINING/VAlIDATION SPLIT\n",
    "#THIS WILL ADD ONTO THE TRAINING AND VALIDATION .jsonl FILES. NOT OVERWRITE THEM.\n",
    "generate_training_and_val_data(path, sep, stop_seq)\n",
    "\n",
    "#RUN THE STUFF BELOW TOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataset(name_jsonl):\n",
    "    with open(name_jsonl, \"r\", encoding = \"utf-8\") as f:\n",
    "        lines = f.read().split(\"\\n\")\n",
    "    f.close()\n",
    "    return list(map( lambda x: json.loads(x), lines[0:-1]))\n",
    "\n",
    "def print_len_dataset(name_jsonl):\n",
    "    print(name_jsonl + \" has length \" + str(len(get_dataset(name_jsonl)))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_new_dataset(name, dataset):\n",
    "    with open(name+\".jsonl\", \"w\", encoding = \"utf-8\") as f:\n",
    "        for i in dataset:\n",
    "            json.dump(i, f)\n",
    "            f.write(\"\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training.jsonl has length 1953\n",
      "\n",
      "validation.jsonl has length 114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = get_dataset(\"training.jsonl\")\n",
    "validation = get_dataset(\"validation.jsonl\")\n",
    "print_len_dataset(\"training.jsonl\")\n",
    "print_len_dataset(\"validation.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Shortened_training is what was actually used for finetuning GPT-3\n",
    "write_new_dataset(\"shortened_training_0\", validation)\n",
    "write_new_dataset(\"shortened_training\", training[0:407])\n",
    "write_new_dataset(\"shortened_validation\",training[407:509])\n",
    "shortened_training = get_dataset(\"shortened_training.jsonl\")\n",
    "shortened_training = get_dataset(\"shortened_validation.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shortened_training.jsonl has length 407\n",
      "\n",
      "shortened_validation.jsonl has length 102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_len_dataset(\"shortened_training.jsonl\")\n",
    "print_len_dataset(\"shortened_validation.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
