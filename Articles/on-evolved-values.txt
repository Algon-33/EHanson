author: Robin Hanson

text: 
Biological evolution selects roughly for creatures that do whatever it takes to have more descendants in the long run. When such creatures have brains, those brains are selected for having supporting habits. And to the extent that such brains can be described as having beliefs and values that combine into actions via expected utility theory, then these beliefs and values should be ones which are roughly behaviorally-equivalent to the package of having accurate beliefs, and having values to produce many descendants (relative to rivals). Equivalent at least within the actual environments in which those creatures were selected.
Humans have unusually general brains, with which we can think unusually abstractly about our beliefs and values. But so far, we haven’t actually abstracted our values very far. We instead have a big mess of opaque habits and desires that implicitly define our values for us, in ways that we poorly understand. Even though what evolution has been selecting for in us can in fact be described concisely and effectively in an abstract way.
Which leads to one of the most disturbing theoretical predictions I know: with sufficient further evolution, our descendants are likely to directly and abstractly know that they simply value more descendants. In diverse and varying environments, such a simpler more abstract representation seems likely to be more effective at helping them figure out which actions would best achieve that value. And while I’ve personally long gotten used to the idea that our distant descendants will be weird, to (the admittedly few) others who care about the distant future, this vision must seem pretty disturbing.
Oh there are some subtleties regarding whether all kinds of long-term descendants get the same weight, to what degree such preferences are non-monotonic in time and number of descendants, and whether we care the same about risks that are correlated or not across descendants. But those are details: evolved descendants should more simply and abstractly value more descendants.
This applies whether our descendants are biological or artificial. And it applies regardless of the kind of environments our descendants face, as long as those environments allow for sufficient selection. For example, if our descendants live among big mobs, who punish them for deviations from mob-enforced norms, then our descendants will be selected for pleasing their mobs. But as an instrumental strategy for producing more descendants. If our descendants have a strong democratic world government that enforces rules about who can reproduce how, then they will be selected for gaining influence over that government in order to gain its favors. And for an autocratic government, they’d be selected for gaining its favors.
Nor does this conclusion change greatly if the units of future selection are larger than individual organisms. Even if entire communities or work teams reproduce together as single units, they’d still be selected for valuing reproduction, both of those entire units and of component parts. And if physical units are co-selected with supporting cultural features, those total physical-plus-cultural packages must still tend to favor the reproduction of all parts of those packages.
Many people seem to be confused about cultural selection, thinking that they are favored by selection if any part of their habits or behaviors is now growing due to their actions. But if, for example, your actions are now contributing to a growing use of the color purple in the world, that doesn’t at all mean that you are winning the evolutionary game. If wider use of purple is not in fact substantially favoring the reproduction of the other elements of the package by which you are now promoting purple’s growth, and if those other elements are in fact reproducing less than their rivals, then you are likely losing, not winning, the evolutionary game. Purple will stop growing and likely decline after those other elements sufficiently decline.
Yes of course, you might decide that you don’t care that much to win this evolutionary game, and are instead content to achieve the values that you now have, with the resources that you can now muster. But you must then accept that tendencies like yours will become a declining fraction of future behavior. You are putting less weight on the future compared to others who focus more on reproduction. The future won’t act like you, or be as much influenced by acts like yours.
For example, there are “altruistic” actions that you might take now to help out civilization overall. For example, you might build a useful bridge, or find some useful invention. But if by such actions you hurt the relative long-term reproduction of many or most of the elements that contributed to your actions, then you must know you are reducing the tendency of descendants to do such actions. Ask: is civilization really better off with more such acts today, but fewer such acts in the future?
Yes, we can likely identify some parts of our current packages which are hurting, not helping, our reproduction. Such as genetic diseases. Or destructive cultural elements. It makes sense to dump such parts of our reproduction “teams” when we can identify them. But that fact doesn’t negate the basic story here: we will mainly value reproduction.
The only way out I see is: stop evolution. Stop, or slow to a crawl, the changes that induce selection of features that influence reproduction. This would require a strong civilization-wide government, and it only works until we meet the other grabby aliens. Worse, in an actually changing universe, such stasis seems to me to seriously risk rot. Leading to a slowly rotting civilization, clinging on to its legacy values but declining in influence, at least relative to its potential. This approach doesn’t at all seems worth the cost to me.
But besides that, have a great day.
Added 7p: There many be many possible equilibria, in which case it may be possible to find an equilibrium in which maximizing reproduction also happens to maximize some other desired set of values. But it may be hard to maintain the context that allows that equilibrium over long time periods. And even if so, the equilibrium might itself drift away to support other values.
Added 8Dec: This basic idea expressed 14 years ago.


title: On Evolved Values

date: December 5, 2021 11:25 am

