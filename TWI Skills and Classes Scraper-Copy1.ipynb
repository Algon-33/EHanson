{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Notes:\n",
    "#1) Instead of pre-processing some data, see if someone has already processed it\n",
    "#2) Instead of doing task X (which you've never done before), see if someone has already done it\n",
    "#3) I really want to make a decorator for python s.t. I can just create progress bars by writing @bar before a for loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_pred(ls, pred):\n",
    "    x=[]\n",
    "    for i in ls:\n",
    "        if pred(i):\n",
    "            x.append(i)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get the chapter for TWI. This should get all the chapters, but you should double check chapter_links = chapter_links[27:-16]\n",
    "#actually gets you up to date. \n",
    "def get_chapter_links():\n",
    "    URL = \"https://wanderinginn.com/table-of-contents/\"\n",
    "    page = requests.get(URL)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "    chapter_links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        chapter_links.append(link.get('href'))\n",
    "    chapter_links = chapter_links[27:-16]\n",
    "    return chapter_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write chapter links to the Chapter Links sub directory, in the file \"chapterlinks.txt\"\n",
    "\n",
    "#os.chdir(path +'\\\\Chapter Links')\n",
    "#chapter_links = get_chapter_links()\n",
    "#with open(\"chapterlinks.txt\",\"w\", encoding=\"utf-8\") as f:\n",
    "#    f.write(\"\\n\".join(chapter_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This was just a pice of code to check that I was writing chapters properly.\n",
    "#os.chdir(path +'\\\\Chapters')\n",
    "#write_chapter(\"https://wanderinginn.com/2017/04/25/1-00-c/\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Write the chapter contained in chapter_link to \\\\Chapters \n",
    "\n",
    "\"\"\"def write_chapter(chapter_link, path):\n",
    "    os.chdir(path +'\\\\Chapters')\n",
    "    chapter_n = requests.get(chapter_link)\n",
    "\n",
    "    soup_chapter_n = BeautifulSoup(chapter_n.content, \"html.parser\")\n",
    "    chapter_n_text = soup_chapter_n.find(\"div\", {\"class\", \"entry-content\"}).get_text()\n",
    "    chapter_n = soup_chapter_n.find(\"h1\", {\"class\", \"entry-title\"}).get_text()\n",
    "    \n",
    "    with open(chapter_n+\".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(chapter_n_text)\n",
    "    f.close()\n",
    "    os.chdir(path)\n",
    "\"\"\"\n",
    "#Writes every chapter to \\\\Chapters\n",
    "\"\"\"    \n",
    "os.chdir(path +'\\\\Chapters')\n",
    "for i in range(0, 0):\n",
    "    write_chapter(chapter_links[i], path)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_pages_txt_files(path):\n",
    "\n",
    "    os.chdir(path +'\\\\Chapters')\n",
    "\n",
    "    chaps = os.listdir()\n",
    "\n",
    "    for i in range(len(chaps)):\n",
    "        os.chdir(path +'\\\\Chapters')\n",
    "        with open(chaps[i], \"r\", encoding = \"utf-8\") as chap:\n",
    "            txt = chap.readlines()\n",
    "        chap.close()\n",
    "        chap_pages_data= break_chap_into_pages(chaps[i], txt)\n",
    "\n",
    "        os.chdir(path +'\\\\Chapter Pages')\n",
    "        for i in range(len(chap_pages_data)):\n",
    "            chapter_name, page_num, page_text = chap_pages_data[i]\n",
    "            with open(chapter_name+\" page \"+str(page_num)+\".txt\", \"w\", encoding = \"utf-8\") as f:\n",
    "                f.write(page_text)\n",
    "            f.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def break_chap_into_pages(chapter_txt, chapter_lines):\n",
    "    chapter_name = chapter_txt.strip(\".txt\")\n",
    "    counter = 900*4\n",
    "    text = \"\"\n",
    "    pages = []\n",
    "    for i in chapter_lines:\n",
    "        if len(text+i)>counter or i ==chapter_lines[-1]:\n",
    "            \n",
    "            pages.append((chapter_name, len(pages), chapter_name+\"\\n\"+\"Page \"+str(len(pages))+\"\\n\"+text))\n",
    "            text=i\n",
    "        else:\n",
    "            text+=i\n",
    "    \n",
    "    return pages\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_pages_txt_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.00 C page 0.txt',\n",
       " '1.00 C page 1.txt',\n",
       " '1.00 C page 10.txt',\n",
       " '1.00 C page 11.txt',\n",
       " '1.00 C page 12.txt',\n",
       " '1.00 C page 13.txt',\n",
       " '1.00 C page 14.txt',\n",
       " '1.00 C page 2.txt',\n",
       " '1.00 C page 3.txt',\n",
       " '1.00 C page 4.txt',\n",
       " '1.00 C page 5.txt',\n",
       " '1.00 C page 6.txt',\n",
       " '1.00 C page 7.txt',\n",
       " '1.00 C page 8.txt',\n",
       " '1.00 C page 9.txt',\n",
       " '1.00 D page 0.txt',\n",
       " '1.00 D page 1.txt',\n",
       " '1.00 D page 10.txt',\n",
       " '1.00 D page 11.txt',\n",
       " '1.00 D page 12.txt',\n",
       " '1.00 D page 13.txt',\n",
       " '1.00 D page 14.txt',\n",
       " '1.00 D page 15.txt',\n",
       " '1.00 D page 16.txt',\n",
       " '1.00 D page 17.txt',\n",
       " '1.00 D page 18.txt',\n",
       " '1.00 D page 2.txt',\n",
       " '1.00 D page 3.txt',\n",
       " '1.00 D page 4.txt',\n",
       " '1.00 D page 5.txt',\n",
       " '1.00 D page 6.txt',\n",
       " '1.00 D page 7.txt',\n",
       " '1.00 D page 8.txt',\n",
       " '1.00 D page 9.txt',\n",
       " '1.00 H page 0.txt',\n",
       " '1.00 H page 1.txt',\n",
       " '1.00 H page 10.txt',\n",
       " '1.00 H page 11.txt',\n",
       " '1.00 H page 12.txt',\n",
       " '1.00 H page 2.txt',\n",
       " '1.00 H page 3.txt',\n",
       " '1.00 H page 4.txt',\n",
       " '1.00 H page 5.txt',\n",
       " '1.00 H page 6.txt',\n",
       " '1.00 H page 7.txt',\n",
       " '1.00 H page 8.txt',\n",
       " '1.00 H page 9.txt',\n",
       " '1.00 page 0.txt',\n",
       " '1.00 page 1.txt',\n",
       " '1.01 C page 0.txt',\n",
       " '1.01 C page 1.txt',\n",
       " '1.01 C page 2.txt',\n",
       " '1.01 C page 3.txt',\n",
       " '1.01 C page 4.txt',\n",
       " '1.01 C page 5.txt',\n",
       " '1.01 C page 6.txt',\n",
       " '1.01 D page 0.txt',\n",
       " '1.01 D page 1.txt',\n",
       " '1.01 D page 10.txt',\n",
       " '1.01 D page 11.txt',\n",
       " '1.01 D page 12.txt',\n",
       " '1.01 D page 13.txt',\n",
       " '1.01 D page 14.txt',\n",
       " '1.01 D page 15.txt',\n",
       " '1.01 D page 16.txt',\n",
       " '1.01 D page 17.txt',\n",
       " '1.01 D page 2.txt',\n",
       " '1.01 D page 3.txt',\n",
       " '1.01 D page 4.txt',\n",
       " '1.01 D page 5.txt',\n",
       " '1.01 D page 6.txt',\n",
       " '1.01 D page 7.txt',\n",
       " '1.01 D page 8.txt',\n",
       " '1.01 D page 9.txt',\n",
       " '1.01 H page 0.txt',\n",
       " '1.01 H page 1.txt',\n",
       " '1.01 H page 10.txt',\n",
       " '1.01 H page 2.txt',\n",
       " '1.01 H page 3.txt',\n",
       " '1.01 H page 4.txt',\n",
       " '1.01 H page 5.txt',\n",
       " '1.01 H page 6.txt',\n",
       " '1.01 H page 7.txt',\n",
       " '1.01 H page 8.txt',\n",
       " '1.01 H page 9.txt',\n",
       " '1.01 page 0.txt',\n",
       " '1.01 page 1.txt',\n",
       " '1.01 page 2.txt',\n",
       " '1.01 page 3.txt',\n",
       " '1.01 page 4.txt',\n",
       " '1.01 R page 0.txt',\n",
       " '1.01 R page 1.txt',\n",
       " '1.01 R page 2.txt',\n",
       " '1.01 R page 3.txt',\n",
       " '1.01 R page 4.txt',\n",
       " '1.01 R page 5.txt',\n",
       " '1.01 R page 6.txt',\n",
       " '1.02 C page 0.txt',\n",
       " '1.02 C page 1.txt',\n",
       " '1.02 C page 10.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(path +'\\\\Chapter Pages')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Attempt 1: This was a SLOW and inelegant way to do things, as the wandering inn wiki has an (exhaustive?) list of [Classes] and\n",
    "#[Skills]\n",
    "#Note that when I did this, I messed up the scraping a bit and duplicated some of the data. So you might not get a superset\n",
    "#of my data if you try running this notebook.\n",
    "\n",
    "import re\n",
    "\n",
    "x = re.findall(\"[\\[]{1}.*?[\\]{1}]\", \"[Deathbolt] spell of the [Necromancer]\")\n",
    "assert x == [\"[Deathbolt]\", \"[Necromancer]\"]\n",
    "get_skills_and_classes = lambda chapter_text: re.findall(\"[\\[]{1}.*?[\\]{1}]\", chapter_text)\n",
    "assert get_skills_and_classes(chapter_n_text)[0] == '[Innkeeper Class Obtained!]'\n",
    "\n",
    "#Uncomment when you run\n",
    "#NN_text = []\n",
    "\n",
    "#I had to iterate through this manually, running the code and incrementing last_n by 30 each time. Why? Because it was taking too\n",
    "#long to just iterate through range(len(chapter_links)) and I was worried that there might be anti scraping measures or something\n",
    "#that prevent me from doing it all repeatedly or whatever. That's probably dumb though.\n",
    "#To run it yourself, set n = 0 and keep going till you hit the largest N s.t. 30 N < len(chapter_links)\n",
    "#Or just change range(30) to range(len(chapter_links)) and get rid of the if clause\n",
    "\n",
    "last_n = 600\n",
    "for n in range(30):\n",
    "    if n+last_n>613:\n",
    "        break\n",
    "    chapter_n = requests.get(chapter_links[n+last_n])\n",
    "\n",
    "    soup_chapter_n = BeautifulSoup(chapter_n.content, \"html.parser\")\n",
    "    chapter_n_text = soup_chapter_n.find(\"div\", {\"class\", \"entry-content\"}).get_text()\n",
    "#Uncomment when you run\n",
    "#    NN_text += list(set(get_skills_and_classes(chapter_n_text)))\n",
    "\n",
    "NN_text[-300:]\n",
    "\n",
    "#Dataset has some announcements and I was too lazy to find whether they had special tags accompnying them in the soup, so I just\n",
    "#made a hacky regex that probably won't remove all the announcements\n",
    "x = re.findall(\"[\\[]{1}([a-zA-Z]|[,])*[\\]{1}]\", \"[Deathbolt] spell of the [Necromancer]\")\n",
    "is_annoucement = lambda txt: 1 if re.search(\".*[eE]\\-book.*|.*Merch.Store.*|.*Author.has.*|.*is now available for.*|.*is out now.*|.*now has a.*|.*filled out the.|.*[Pp]atreon.*|.*has released.*\", txt) !=None else 0\n",
    "NN_no_announcements = []\n",
    "for i in range(len(NN_text)):\n",
    "    if not is_annoucement(NN_text[i]):\n",
    "        NN_no_announcements.append(NN_text[i])\n",
    "#Unless something has gone wrong, TWI should have a couple of things which match is_announcement and so NN_no_announcements\n",
    "#should be a strict sublist of NN_text\n",
    "assert len(NN_no_announcements) < len(NN_text)\n",
    "\n",
    "is_def_skill = lambda txt: 1 if re.search(\".*[sS]kill.*\",txt)!=None  else 0\n",
    "\n",
    "assert is_def_skill('[Undying Lich, Myth of Death and Vengeance Level 78!]') == 0\n",
    "assert is_def_skill(\"[Necromancer]\")==False\n",
    "\n",
    "NN_def_skills = []\n",
    "for i in NN_no_announcements:\n",
    "    if is_def_skill(i):\n",
    "        NN_def_skills.append(i)\n",
    "NN_def_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Attempt 2: Way less complicated\n",
    "#Lesson learnt: Why clean the data if someone else has already done it for you?\n",
    "page_skill = requests.get(\"https://thewanderinginn.fandom.com/wiki/Skills\")\n",
    "soup_skill = BeautifulSoup(page_skill.content, \"html.parser\")\n",
    "page_class = requests.get(\"https://thewanderinginn.fandom.com/wiki/Classes\")\n",
    "soup_class= BeautifulSoup(page_class.content, \"html.parser\")\n",
    "page_spell = requests.get(\"https://thewanderinginn.fandom.com/wiki/Spells\")\n",
    "soup_spell= BeautifulSoup(page_spell.content, \"html.parser\")\n",
    "\n",
    "\n",
    "skills = []\n",
    "classes = []\n",
    "spells = []\n",
    "soup_skill_tables = soup_skill.find_all(\"table\", {\"class\", \"wikitable\"})\n",
    "is_skill = lambda txt: 1 if re.search(\"^\\[.*\\]\\n.*\", txt) !=None and re.search(\".*\\[[0-9]*\\].*\",txt) ==None else 0\n",
    "soup_skill_td = soup_skill.find_all(\"td\")\n",
    "for i in soup_skill_td:\n",
    "    if is_skill(i.get_text()):\n",
    "        skills.append(i.get_text())\n",
    "\n",
    "\n",
    "soup_class_tables = soup_class.find_all(\"table\", {\"class\", \"wikitable\"})\n",
    "is_class = lambda txt: 1 if re.search(\"^\\[.*\\]\\n.*\", txt) !=None and re.search(\".*\\[[0-9]*\\].*\",txt) ==None else 0\n",
    "soup_class_td = soup_class.find_all(\"td\")\n",
    "for i in soup_class_td:\n",
    "    if is_class(i.get_text()):\n",
    "        classes.append(i.get_text())\n",
    "\n",
    "is_spell = lambda txt: 1 if re.search(\"^\\[.*\\]\\n.*\", txt) !=None and re.search(\".*\\[[0-9]*\\].*\",txt) ==None else 0\n",
    "\n",
    "def turn_into_spell(txt):\n",
    "    \n",
    "    if txt[0]!=\"[\":\n",
    "        txt = \"[\"+txt\n",
    "    if txt[-1]!=\"]\":\n",
    "        txt+=\"]\"\n",
    "    return txt\n",
    "\n",
    "soup_spell_td = soup_spell.find_all(\"td\")\n",
    "for i in soup_spell_td:\n",
    "    if is_spell(i.get_text()):\n",
    "        i_txt = i.get_text()\n",
    "        sep_i = i_txt.split(\"][\")\n",
    "        if len(sep_i) ==1:\n",
    "            spells+=sep_i\n",
    "        else:\n",
    "            sep_i = [turn_into_spell(j) for j in sep_i]\n",
    "            spells+=sep_i\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skill_set = set([i.strip(\"\\n\") for i in skills])\n",
    "spell_set = set([i.strip(\"\\n\") for i in spells])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "is_change = lambda txt: 1 if re.search(\".*→.*\",txt)!=None else 0\n",
    "clean_txt = lambda pattern, repl, txt: re.sub(pattern, repl, txt)\n",
    "\n",
    "#Note that I had to get rid of stuff like [Conditions Met: Goth → Abyssal W—] or else I'd have been there for a while\n",
    "def get_NN_clean(NN_no_announcements):\n",
    "    NN_split = []\n",
    "    for i in NN_no_announcements:\n",
    "        if is_change(i):\n",
    "            x = i.split(\"→\")\n",
    "            x = [turn_into_spell(j) for j in x]\n",
    "            NN_split+= x\n",
    "        else:\n",
    "            NN_split.append(i)\n",
    "    \n",
    "    NN_clean = []\n",
    "    for i in NN_split:\n",
    "        x = clean_txt(\"...+\\[.*\\]|.* Users .*[Cc]onnected.|.* has disconnected.*|.*joined the conversation.*|.*[a-zA-Z]+[0-9]+.*|.*[0-9]+[a-zA-Z]+.*|.*[a-zA-Z]+_[0-9]+.*|.*batman.|\\[.\\]\", \"\", i)\n",
    "        x = clean_txt(\">.*\", \"\\]\", x)\n",
    "        if x!=\"[]\" and x!=\"\" :\n",
    "            NN_clean.append(turn_into_spell(x))\n",
    "    return NN_clean\n",
    "NN_clean = get_NN_clean(NN_no_announcements)\n",
    "set(NN_clean)\n",
    "len(NN_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\".*[Cc]onditions [Mm]et :|[a-zA-Z]*[0-9]+|^Skill -|Ob[—-]|Skill – |Ski[-—]||Level [0-9]!*|Level[sing]*| Learned.|Ob[-—]|Class | acquired |[Oo]btained!|\"\n",
    "def get_potential_NN_classes(NN_clean, spells_set, skills_set):\n",
    "    temp = []\n",
    "    for i in NN_clean:\n",
    "        x = clean_txt(\"|Ritual|[Ll]esser [Bb]ond|learned|Lesser Bond|removed|Weapon Art|Artform|Miracle [\\—\\-\\–]|siphoned|Empowered Spell: |\\.|Aura Binding:|(Bind|Bound) Spell:|(temporary)|granted.|received|Legacy [\\—\\-\\–] |acquired|!|lost.|Spell [\\—\\-\\–]* |Temporary Skill Assigned:|Temporary Skill|Consolidation Failed. Unable to Advance|Condition [\\—\\-\\–]|[Cc]onditions* [Mm]et:|[a-zA-Z]*[0-9]+|^Skill -|Ob[\\—\\-\\–]|Skill [\\—\\-\\–] |Ski[\\—\\-\\–]|Level [0-9]!*|Level|Level[sing]*| Learned.|Ob[\\—\\-\\–]|Class | acquired |[Oo]btained!|Consolidation:|.*~.*~.*|\\\\{2}|Condition [\\—\\-\\–]|[Cc]onditions* [Mm]et:|[a-zA-Z]*[0-9]+|^Skill -|Ob[\\—\\-\\–]|Skill [\\—\\-\\–] |Ski[\\—\\-\\–]|Level [0-9]!*|Level|Level[sing]*| Learned.|Ob[\\—\\-\\–]|Class | acquired |[Oo]btained!*| !\", \"\", i)\n",
    "        x = clean_txt(\"\\[ \", \"\\[\",x)\n",
    "        x = clean_txt(\" \\]\", \"\\]\",x)\n",
    "        x = x.replace(\"\\\\\",\"\")\n",
    "        temp.append(x)\n",
    "        \n",
    "    temp = set(temp)\n",
    "    temp = temp.difference(spells_set)\n",
    "    temp = temp.difference(skills_set)\n",
    "    \n",
    "    return temp\n",
    "class_set = get_potential_NN_classes(NN_clean, spell_set, skill_set)\n",
    "class_txt = \"\\n\".join([i for i in list(class_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"spells.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\".join(spells))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"classes_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "#temp = lambda txt: clean_txt(\"!| [Oo]btained|Level |levelled|[0-9]+|.*[\\—\\-\\–].*|.*[Ss]kill.*\" ,\"\", txt)\n",
    "def temp_1(lines):\n",
    "    temp = lambda txt: clean_txt(\".*Rank  Horror.*|Temporary|.*:.*|[Rr]emoved|Class|.*Boon of the.*|.*\\(.*|Aspect.*|\\.|My Noble Virtue:.*|Bound Spell:.*|Class Consolidation Failed. Unable to Advance|^\\[\\\\n$|^\\[\\].*|^\\[$|Class Consolidation: |Spell [\\—\\-\\–].*|Condition.*|Conditions Met.*|.*Miracle.*|!| [Oo]btained|Level |levelled|[0-9]+|.*[\\—\\-\\–]{1}\\]|.*[Ss]kill.*\" ,\"\", txt)\n",
    "    remove_extra_spaces = lambda txt: clean_txt(\"\\[ \",\"[\" ,clean_txt(\" \\]\", \"]\", txt))\n",
    "    x = [];\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i]\n",
    "        if temp(line) != \"\\n\":\n",
    "            x.append(remove_extra_spaces(temp(line)))\n",
    "    return x\n",
    "new_lines = list(map(lambda x: x+\"\\n\",  list(set(map( lambda x: x.strip(\"\\n\"), temp_1(lines))).difference(spell_set).difference(skill_set))))\n",
    "with open(\"classes_cleaned.txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "    f.write(\"\".join(new_lines))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"dataset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = f.readlines()\n",
    "f.close()\n",
    "\n",
    "\n",
    "#dataset = list(map(lambda x:\" \".join(x.lower().replace(\"-\", \" \").replace(\"–\", \"\").replace(\"—\", \" \").split()), dataset))\n",
    "\n",
    "with open(\"dataset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(dataset))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(\"\".join(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
